# Sankalp_Chauhan_Sonny_Portfolio
Data Science Portfolio

# [Project 1: Dognition Data Final Project](https://public.tableau.com/profile/sankalp.chauhan5511#!/vizhome/CourseraTableauFinalDognitionAnalysis/Story1)
## Overview
 This was the final assignment for my Coursera Course on Data Visualization and Communication with Tableau.
I was given a Dognition Data Set on the users of the Dognition Service(games for dogs)and I had to create a Tableau visualization inorder to improve participate in the Dognition 
services. 
To accomplish this task, I made several graphs and charts in Tableau showcasing:
A line chart with the  membership data for the years 2013,2014, and 2015.  
A map of the dognition data based on breed of dogs, and gender.
A pie chart of the cities of dognition users.
A bar graph with breed types, breed groups, and male/female dogs and 
A bar chart showcasing the games. 
Lastly, I wrote the recommendations for how to improve the Dognition service.

# [Project 2: Rensselaer Polytechnic Institute (RPI) Summer Undergraduate Research Program (SURP) project Researcher](https://github.com/Sankalp1233/Rensselaer-Polytechnic-Institute-RPI-Summer-Undergraduate-Research-Program-SURP-Project)
## Overview
     I created a multiwindow Shiny app which uses New York City open government database to display the conditions of the trees of New York City. The dataset used was the 2015 
Street Tree Census- Tree Data that contained information about trees in New York City from the five boroughs (Bronx, Brooklyn, Queens, Staten Island, and Manhattan) such as tree 
species, health of the tree, Latin name of the tree, common name of the tree, latitude and longitude of the tree, and other important information.  From the dataset available, I 
enriched the datasets by boroughs to structure the features that were important in the study: unique tree identifier (tree id), latitude, longitude, borough of the tree, the 
health of the tree, the Latin and English names of the tree, and zip code of the tree to use for further analysis. 
      From the datasets the unique Latin names of the trees were made into a data frame. From this new data frame, Wikipedia pages were added corresponding with the Latin names 
of the tree, making a second column in the data frame. After extensive code development to add the new column to the dataset, and debugging the code, the Wikipedia pages for the 
Latin name of the trees were added and were given the column name tour text in order to help build the framework for the multiwindow Shiny app. These Wikipedia urls were 
ingested into the multiwindow Shiny app and then shown in one of the external monitors in the Campfire. The IDEA Campfire comprises of desk-height, 10-foot panoramic screen (the 
Wall) and floor projection (Floor) with no artificial or virtual barriers, provided a great environment for simultaneously showing different aspects of the application and data. 
The IDEA campfire has the power of linking data for simultaneously accessing and displaying as much information about the trees as possible. The multi window shiny app with 
integrated features can be used to display the conditions of the trees, thus facilitating the task of monitoring the green spaces and their enhancement in the urban setting.  
From the NYC open data base, URIs (uniform resource identifiers) were constructed and linked to Wikipedia pages and physical location-based web APIs (Google Street, the street-
level panorama).


# [Project 3: Computational Optimization Final Project](https://github.com/Sankalp1233/Computational-Optimization-Final-Project)
## Overview
In this course Computational Optimization, I studied different optimization methods such as Steepest gradient descent, Conjugate Gradient Method, Conjugate Direction Method, BB 
method, gradient descent, steepest gradient descent, augmented lagrangian function, and augmented lagrangian function with quadratic penalty. I learned how these algorithms 
terminate (stop) and how to code these algorithms in Matlab. In the final project of this class, an augmented Lagrangian function (an optimization algorithm) was constructed in 
order to separate positive and negative data points using soft margin Support Vector Machines (SVM) a machine learning algorithm and tested for accuracy of the data set. Another 
augmented Lagrangian function was constructed to minimize false negative errors while controlling false positive errors using Neyman Pearson Classification. The final project 
was made using LATEX to create a pdf document.
